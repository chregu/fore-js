import e from"axios";import{v4 as t}from"uuid";const r={GROUNDEDNESS:"GROUNDEDNESS",SIMILARITY:"SIMILARITY"};class s{constructor({apiToken:e,apiUrl:t="https://foresight-userservice-dev.azurewebsites.net",uiUrl:r="https://icy-sand.foreai.co",maxEntriesBeforeAutoFlush:s=10,logLevel:n="info"}){this.apiToken=e,this.apiUrl=t,this.uiUrl=r,this.maxEntriesBeforeAutoFlush=s,this.timeoutSeconds=60,this.logEntries=[],this.logging=console,this.logging.info("Foresight client initialized")}async _makeRequest({method:t,endpoint:r,params:s=null,inputJson:n=null}){try{return(await e({method:t,url:`${this.apiUrl}${r}`,headers:{Authorization:`Bearer ${this.apiToken}`},params:s,data:n,timeout:1e3*this.timeoutSeconds})).data}catch(e){throw e}}async createSimpleEvalset({evalsetId:e,queries:r,referenceAnswers:s=null}){try{if(null==e||null==r)throw new Error("evalsetId and queries are required.");if(s&&r.length!==s.length)throw new Error("Number of queries and references must match.");const n={evalset_id:e,evalset_entries:r.map(((e,r)=>({query:e,reference_answer:s?s[r]:null,entry_id:t()})))},i=await this._makeRequest({method:"post",endpoint:"/api/eval/set",inputJson:n});return this.logging.info(`Eval set with evalsetId ${e} created.`),i}catch(e){const t=e.message;throw this.logging.error("createSimpleEvalset:error:",t),new Error(t)}}async getEvalset({evalsetId:e}){try{return await this._makeRequest({method:"get",endpoint:"/api/eval/set",params:{evalset_id:e}})}catch(e){const t=e.message;throw this.logging.error("getEvalset:error:",t),new Error(t)}}async getEvalrunQueries({experimentId:e}){try{return await this._makeRequest({method:"get",endpoint:"/api/eval/run/queries",params:{experiment_id:e}})}catch(e){const t=e.message;throw this.logging.error("getEvalrunQueries:error:",t),new Error(t)}}async createEvalrun({runConfig:e}){try{const t=await this._makeRequest({method:"post",endpoint:"/api/eval/run",inputJson:{evalset_id:e.evalsetId,experiment_id:e.experimentId,metrics:e.metrics}});return this.logging.info(`Eval run with experimentId ${e.experimentId} created.`),t}catch(e){const t=e.message;throw this.logging.error("createEvalrun:error:",t),new Error(t)}}async generateAnswersAndRunEval({generateFn:e,runConfig:t}){try{await this.createEvalrun({runConfig:t});const r=t.experimentId,s=await this.getEvalrunQueries({experimentId:r}),n={};for(const[t,r]of Object.entries(s)){const{generatedResponse:s,contexts:i}=e(r);n[t]={generated_response:s,contexts:i}}const i={experiment_id:r,entry_id_to_inference_output:n},o=await this._makeRequest({method:"put",endpoint:"/api/eval/run/entries",inputJson:i});return this.logging.info("Eval run successful. Visit %s to view results.",this.uiUrl),o}catch(e){const t=e.message;throw this.logging.error("generateAnswersAndRunEval:error:",t),new Error(t)}}async flush(){try{if(0===this.logEntries.length)return void this.logging.info("No log entries to flush.");const e={log_entries:this.logEntries},t=await this._makeRequest({method:"put",endpoint:"/api/eval/log",inputJson:e});return this.logging.log("Log entries flushed successfully. Visit %s to view results.",this.uiUrl),this.logEntries=[],t}catch(e){const t=e.message;throw this.logging.error("flush:error:",t),new Error(t)}}log({query:e,response:t,contexts:r}){try{const s={query:e,inference_output:{generated_response:t,contexts:r}};this.logEntries.push(s),this.logEntries.length>=this.maxEntriesBeforeAutoFlush&&this.flush()}catch(e){const t=e.message;throw this.logging.error("log:error:",t),new Error(t)}}async _convertEvalRunDetailsToDataFrame(e){try{const t={query:[],reference_answer:[],generated_answer:[],source_docids:[],contexts:[]},s=[r.GROUNDEDNESS,r.SIMILARITY];for(const e of s)t[e.toLowerCase()]=[];for(const r of e.entries){t.query.push(r.input.query),t.reference_answer.push(r.input.reference_answer),t.generated_answer.push(r.output.generated_response),t.source_docids.push(r.output.source_docids),t.contexts.push(r.output.contexts);for(const e of s)e in r.metric_values?t[e.toLowerCase()].push(r.metric_values[e]):t[e.toLowerCase()].push(null)}return null}catch(e){throw e}}async getEvalrunDetails({experimentId:e,sortBy:t="input.query",limit:r=100,convertToDataframe:s=!0}){try{const n={experiment_id:e};null!==r&&null!==t&&(n.sort_field_name=t,n.limit=r.toString());const i=await this._makeRequest({method:"get",endpoint:"/api/eval/run/details",params:n});return s?await this._convertEvalRunDetailsToDataFrame(i):i}catch(e){const t=e.message;throw this.logging.error("getEvalrunDetails:error:",t),new Error(t)}}}export{s as Foresight,r as MetricType};
